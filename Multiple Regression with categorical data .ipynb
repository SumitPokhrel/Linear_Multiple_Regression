{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two most important resources for Multiple Regression \n",
    "https://www.youtube.com/watch?v=fTfMdCQJz4s for lecture \n",
    "<br> https://www.youtube.com/watch?time_continue=1553&v=NUXdtN1W1FE for python tutorial "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries \n",
    "import numpy as np # numerical comuputation \n",
    "import matplotlib.pyplot as plt # 2D plotting library  \n",
    "import pandas as pd # for dataframe, 2 D data with rows and columns \n",
    "import seaborn as sns # Seaborn is a library for making statistical graphics in Python. \n",
    "#It is built on top of matplotlib and closely integrated with pandas data structures.\n",
    "%matplotlib inline \n",
    "# The output of plotting commands is displayed inline within frontends like the Jupyter notebook, \n",
    "# directly below the code cell that produced it. The resulting plots will then also be stored in the notebook document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>State</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>165349.20</td>\n",
       "      <td>136897.80</td>\n",
       "      <td>471784.10</td>\n",
       "      <td>New York</td>\n",
       "      <td>192261.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>162597.70</td>\n",
       "      <td>151377.59</td>\n",
       "      <td>443898.53</td>\n",
       "      <td>California</td>\n",
       "      <td>191792.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>153441.51</td>\n",
       "      <td>101145.55</td>\n",
       "      <td>407934.54</td>\n",
       "      <td>Florida</td>\n",
       "      <td>191050.39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>144372.41</td>\n",
       "      <td>118671.85</td>\n",
       "      <td>383199.62</td>\n",
       "      <td>New York</td>\n",
       "      <td>182901.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>142107.34</td>\n",
       "      <td>91391.77</td>\n",
       "      <td>366168.42</td>\n",
       "      <td>Florida</td>\n",
       "      <td>166187.94</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   R&D Spend  Administration  Marketing Spend       State     Profit\n",
       "0  165349.20       136897.80        471784.10    New York  192261.83\n",
       "1  162597.70       151377.59        443898.53  California  191792.06\n",
       "2  153441.51       101145.55        407934.54     Florida  191050.39\n",
       "3  144372.41       118671.85        383199.62    New York  182901.99\n",
       "4  142107.34        91391.77        366168.42     Florida  166187.94"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing the dataset \n",
    "companies = pd.read_csv('1000_Companies.csv')\n",
    "companies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[165349.2 136897.8 471784.1 'New York']\n",
      " [162597.7 151377.59 443898.53 'California']\n",
      " [153441.51 101145.55 407934.54 'Florida']\n",
      " ...\n",
      " [100275.47 241926.31 227142.82 'California']\n",
      " [128456.23 321652.14 281692.32 'California']\n",
      " [161181.72 270939.86 295442.17 'New York']]\n"
     ]
    }
   ],
   "source": [
    "# Extracting the Independent and Dependent variables\n",
    "# By convention X should be uppercase and y should be lowercase \n",
    "X = companies.iloc[:, :-1].values # all rows, all columns except last\n",
    "y = companies.iloc[:, 4].values # all rows, fifth column (starts from 0, which is R&D Spend)\n",
    "print (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <th>Administration</th>\n",
       "      <th>Marketing Spend</th>\n",
       "      <th>Profit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>R&amp;D Spend</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.582434</td>\n",
       "      <td>0.978407</td>\n",
       "      <td>0.945245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Administration</th>\n",
       "      <td>0.582434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.520465</td>\n",
       "      <td>0.741560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marketing Spend</th>\n",
       "      <td>0.978407</td>\n",
       "      <td>0.520465</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.917270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Profit</th>\n",
       "      <td>0.945245</td>\n",
       "      <td>0.741560</td>\n",
       "      <td>0.917270</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 R&D Spend  Administration  Marketing Spend    Profit\n",
       "R&D Spend         1.000000        0.582434         0.978407  0.945245\n",
       "Administration    0.582434        1.000000         0.520465  0.741560\n",
       "Marketing Spend   0.978407        0.520465         1.000000  0.917270\n",
       "Profit            0.945245        0.741560         0.917270  1.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Building the Correlation Matrix \n",
    "companies.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[165349.2 136897.8 471784.1 2]\n",
      " [162597.7 151377.59 443898.53 0]\n",
      " [153441.51 101145.55 407934.54 1]\n",
      " ...\n",
      " [100275.47 241926.31 227142.82 0]\n",
      " [128456.23 321652.14 281692.32 0]\n",
      " [161181.72 270939.86 295442.17 2]]\n"
     ]
    }
   ],
   "source": [
    "# Encoding categorical data of State (New York, California and Florida)\n",
    "# LabelEncoder: ----------------------------------------------------------------\n",
    "# LabelEncoder is used to encode labels with value between 0 and n_classes-1\n",
    "# It can be used to transform non-numerical labels to numerical labels \n",
    "# ------------------------------------------------------------------------------\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "labelencoder = LabelEncoder() # creating a labelencoder object of class LabelEncoder\n",
    "X[:, 3] = labelencoder.fit_transform(X[:, 3]) # New York = 0, California = 1, Florida = 2 \n",
    "# picking the column that needs to be labeled only \n",
    "print (X) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE  LabelEncoder and OneHotEncoder \n",
    "Label Encoding is not good enough. If New York = 0, California = 1, Florida = 2 \n",
    "Say supposing your model internally calculates average This implies that: Average of New York and Florida is California. \n",
    "<br>This is definitely a recipe for disaster.\n",
    "<br> LabelEncoder can turn [dog,cat,dog,mouse,cat] into [1,2,1,3,2], but then the imposed ordinality means that the average \n",
    "of dog and mouse is cat. Still there are algorithms like decision trees and random forests that can work with categorical \n",
    "variables just fine and LabelEncoder can be used to store values using less disk space.One-Hot-Encoding has a the advantage\n",
    "that the result is binary rather than ordinal and that everything sits in an orthogonal vector space. \n",
    "The disadvantage is that for high cardinality, the feature space can really blow up quickly and you start fighting \n",
    "with the curse of dimensionality.\n",
    "<br> For more information ------------------------------------------------------------------------------------------------------\n",
    "<br> https://hackernoon.com/what-is-one-hot-encoding-why-and-when-do-you-have-to-use-it-e3c6186d008f\n",
    "<br> https://datascience.stackexchange.com/questions/9443/when-to-use-one-hot-encoding-vs-labelencoder-vs-dictvectorizor\n",
    "\n",
    "<br> One hot encoder takes only integer labels. So we must use LabelEncoder first "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 0.0000000e+00 1.0000000e+00 1.6534920e+05 1.3689780e+05\n",
      "  4.7178410e+05]\n",
      " [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.6259770e+05 1.5137759e+05\n",
      "  4.4389853e+05]\n",
      " [0.0000000e+00 1.0000000e+00 0.0000000e+00 1.5344151e+05 1.0114555e+05\n",
      "  4.0793454e+05]\n",
      " ...\n",
      " [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.0027547e+05 2.4192631e+05\n",
      "  2.2714282e+05]\n",
      " [1.0000000e+00 0.0000000e+00 0.0000000e+00 1.2845623e+05 3.2165214e+05\n",
      "  2.8169232e+05]\n",
      " [0.0000000e+00 0.0000000e+00 1.0000000e+00 1.6118172e+05 2.7093986e+05\n",
      "  2.9544217e+05]]\n"
     ]
    }
   ],
   "source": [
    "# OneHotEncoder -------------------------------------------------------------------------------------------------------------\n",
    "# OneHotEncoder encodes label encoding into binary form from ordinal \n",
    "#  (New York, California and Florida) into integers (0, 1 and 2) using LabelEncoder\n",
    "# and integers (0, 1 and 2) into binary (1,0), (0,1) and (0,0) using OneHotEncoder\n",
    "# One hot encoder takes only integer labels. So we must use LabelEncoder first  \n",
    "# two dummy variables are needed for 3 types of categorical data (n-1)\n",
    "# New York = (1, 0), California = (0, 1), Florida = (0, 0)\n",
    "onehotencoder = OneHotEncoder(categorical_features =[3]) # categorical_features = column of dataFrame that needs to be HotEncoded only\n",
    "# Since 4th (0,1,2,3) column has label encoded data in the dataFrame\n",
    "# Unlike labelEncoder, we have to give the entitre dataFrame as a parameter to OneHotEncoder\n",
    "X = onehotencoder.fit_transform(X).toarray() # transform the encoded data to an array \n",
    "print (X) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.0000000e+00 1.0000000e+00 1.6534920e+05 1.3689780e+05 4.7178410e+05]\n",
      " [0.0000000e+00 0.0000000e+00 1.6259770e+05 1.5137759e+05 4.4389853e+05]\n",
      " [1.0000000e+00 0.0000000e+00 1.5344151e+05 1.0114555e+05 4.0793454e+05]\n",
      " ...\n",
      " [0.0000000e+00 0.0000000e+00 1.0027547e+05 2.4192631e+05 2.2714282e+05]\n",
      " [0.0000000e+00 0.0000000e+00 1.2845623e+05 3.2165214e+05 2.8169232e+05]\n",
      " [0.0000000e+00 1.0000000e+00 1.6118172e+05 2.7093986e+05 2.9544217e+05]]\n"
     ]
    }
   ],
   "source": [
    "# One hot encoder automatically generates  1 extra dummy variable  \n",
    "# We need only two dummy variables (0,1) for 3 types of categorical data (n-1), where n = no of categories \n",
    "# New York = 1, 0  California = 0, 1  Florida = 0, 0 \n",
    "X = X[:, 1:] # all rows, 2nd to last column (0 is the 1st column)\n",
    "# All this is doing is removing the first column \n",
    "print (X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set \n",
    "from sklearn.model_selection import train_test_split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0) \n",
    "# Convention --> (X,y) X_train, X_test, y_train, y_test \n",
    "# test_size = 0.2, 20% of the total data would be used for testing, rest for training \n",
    "# random_state:===================================================================================================== \n",
    "# If you don't specify the random_state in your code, then every time you run(execute) your code a new \n",
    "# random value is generated and the train and test datasets would have different values each time.\n",
    "# However, if a fixed value is assigned like random_state = 42 (or 0) then no matter how many times\n",
    "# you execute your code the result would be the same .i.e, same values in train and test datasets.\n",
    "# In practice I would say, you should set the random_state to some fixed number while you test stuff, \n",
    "# but then remove it in production if you really need a random (and not a fixed) split.\n",
    "# =================================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Multiple Linear Regression to the Training set \n",
    "from sklearn.linear_model import LinearRegression \n",
    "regressor = LinearRegression() # regressor object of the class LinearRegression \n",
    "regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 89790.61532914  88427.0718736   94894.67836971 175680.86725613\n",
      "  83411.73042087 110571.90200074 132145.2293644   91473.37719685\n",
      " 164597.05380608  53222.82667398  66950.19050988 150566.43987006\n",
      " 126915.20858596  59337.85971048 177513.91053064  75316.28143049\n",
      " 118248.14406603 164574.40699904 170937.28981071 182069.11645087\n",
      " 118845.03252688  85669.95112227 180992.59396146  84145.08220143\n",
      " 105005.83769213 101233.56772746  53831.07669088  56881.41475222\n",
      "  68896.39346903 210040.00765886 120778.72270894 111724.87157654\n",
      " 101487.90541517 137959.02649624  63969.95996741 108857.91214126\n",
      " 186014.7253199  171442.64130749 174644.26529207 117671.49128195\n",
      "  96731.37857432 165452.25779411 107724.34331255  50194.54176911\n",
      " 116513.89532178  58632.48986818 158416.46827611  78541.48521608\n",
      " 159727.66671745 131137.87699644 184880.70924519 174609.08266882\n",
      "  93745.66352058  78341.13383416 180745.90439082  84461.61490551\n",
      " 142900.90602904 170618.44098399  84365.09530837 105307.3716218\n",
      " 141660.07290787  52527.34340439 141842.96264161 139176.27973196\n",
      "  98294.52669665 113586.86790969 126754.21895489 152135.51985563\n",
      "  58864.51658953 174285.57361132 124624.04380784 169065.7765898\n",
      "  91279.33198208 156170.37268964  84307.26579364  77877.75223095\n",
      " 120414.02421346  93380.4427324  139020.62514122 143604.67103574\n",
      " 171148.3081537  140082.97050132 106369.71689746 155641.43851389\n",
      " 140030.10330038 110172.87893524  69672.98677563  88148.5206804\n",
      " 140133.59925094 148479.09537889 157916.63505259  58532.94863139\n",
      "  93707.38422389 112646.37475705  56556.18943658 107414.89996181\n",
      " 147352.80227754 152144.10104035 167808.11701786 118750.25230713\n",
      " 120763.27666701 139029.95295663 157527.90934121 121962.0621496\n",
      "  87091.32399735 104792.91384333  95335.22679184 178389.52287438\n",
      " 181942.63776384 109831.34945506 165254.03344098 167806.06491904\n",
      " 158002.44642545 174782.86900958 170196.77102701  52302.18161609\n",
      " 176938.11595792 104751.83583865  82710.31528804 138890.52767845\n",
      " 144274.74675426 161679.01836442 169662.05445897 120450.9231013\n",
      " 158880.70799548 110213.73252824 169674.51532368  60760.61300839\n",
      " 159036.99629069 158169.44286049 174511.70494477 156294.79927785\n",
      " 103714.37583211  85635.96237573 141603.54878758 165917.69156982\n",
      " 121182.03641977 170751.87883895 100505.77549411  82097.51033126\n",
      " 178643.18879845 101790.48384578  70507.4095862   90250.04230087\n",
      "  61247.49962678  68912.17534518  72775.81613474 176914.08873127\n",
      "  89704.6924493  129209.43730015  92672.90938382  88133.59175043\n",
      " 172836.33021621  60893.62070012 169015.89446012 166450.24453206\n",
      " 165425.54476417 102170.51694989 181594.57928218  73702.5794256\n",
      "  91267.42979667 135791.54160196  64922.80257298  71775.70235724\n",
      "  60603.91401513 184288.61041918 176286.69585947 158907.7568704\n",
      " 141359.32216439 154611.17928322  58549.58863231  90618.58407898\n",
      " 152885.51163926 168398.05223807  72485.36274537 116064.24350667\n",
      "  80087.80697207 149828.90896189 116806.9595737  130191.48845161\n",
      " 174534.42670331 293584.45948285 146270.8317479  150646.69178015\n",
      "  86107.47782246  69967.20842244  70096.78368771  69033.69170767\n",
      " 120666.75708064  89677.68014062 166824.27091664 125514.7662641\n",
      "  67209.67687464 140930.69427704 118544.30490695 165897.61905908\n",
      " 168655.48652554 147009.6680505  141396.22104147 109086.50634848]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicting the Test set results \n",
    "y_pred = regressor.predict(X_test)\n",
    "print (y_pred)\n",
    "y_pred.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-8.80536598e+02 -6.98169073e+02  5.25845857e-01  8.44390881e-01\n",
      "  1.07574255e-01]\n"
     ]
    }
   ],
   "source": [
    "# Calculating the Coefficients \n",
    "print (regressor.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on number of coeffients and independent variables \n",
    "Remember there are 5 coefficients and one intercept. \n",
    "<br> y   =  m1x1 + m2x2 + m3x3 + m4x4 + m5x5 + C \n",
    "<br> Profit = R&D Spend, Administration,\tMarketing Spend, State (3 different states) \n",
    "<br> We have three different states but we need only 2 coefficients ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-51035.22972405603\n"
     ]
    }
   ],
   "source": [
    "# Calculating the Intercept \n",
    "print (regressor.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9112695892268936"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating the R squared value \n",
    "from sklearn.metrics import r2_score \n",
    "r2_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's use Statsmodels and see the multiple regression summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>         <td>y_train</td>     <th>  R-squared:         </th> <td>   0.959</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.958</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   3672.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 08 Aug 2018</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>09:13:36</td>     <th>  Log-Likelihood:    </th> <td> -8375.2</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   800</td>      <th>  AIC:               </th> <td>1.676e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   794</td>      <th>  BIC:               </th> <td>1.679e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     5</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "       <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>  <td>-5.104e+04</td> <td> 3998.483</td> <td>  -12.764</td> <td> 0.000</td> <td>-5.89e+04</td> <td>-4.32e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X_train[0]</th> <td> -880.5366</td> <td>  738.404</td> <td>   -1.192</td> <td> 0.233</td> <td>-2329.991</td> <td>  568.917</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X_train[1]</th> <td> -698.1691</td> <td>  737.843</td> <td>   -0.946</td> <td> 0.344</td> <td>-2146.523</td> <td>  750.185</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X_train[2]</th> <td>    0.5258</td> <td>    0.034</td> <td>   15.523</td> <td> 0.000</td> <td>    0.459</td> <td>    0.592</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X_train[3]</th> <td>    0.8444</td> <td>    0.031</td> <td>   26.983</td> <td> 0.000</td> <td>    0.783</td> <td>    0.906</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>X_train[4]</th> <td>    0.1076</td> <td>    0.016</td> <td>    6.548</td> <td> 0.000</td> <td>    0.075</td> <td>    0.140</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>1508.437</td> <th>  Durbin-Watson:     </th>  <td>   2.004</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>2536304.512</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td>12.949</td>  <th>  Prob(JB):          </th>  <td>    0.00</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td>277.624</td> <th>  Cond. No.          </th>  <td>3.76e+06</td>  \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 3.76e+06. This might indicate that there are<br/>strong multicollinearity or other numerical problems."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                y_train   R-squared:                       0.959\n",
       "Model:                            OLS   Adj. R-squared:                  0.958\n",
       "Method:                 Least Squares   F-statistic:                     3672.\n",
       "Date:                Wed, 08 Aug 2018   Prob (F-statistic):               0.00\n",
       "Time:                        09:13:36   Log-Likelihood:                -8375.2\n",
       "No. Observations:                 800   AIC:                         1.676e+04\n",
       "Df Residuals:                     794   BIC:                         1.679e+04\n",
       "Df Model:                           5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept  -5.104e+04   3998.483    -12.764      0.000   -5.89e+04   -4.32e+04\n",
       "X_train[0]  -880.5366    738.404     -1.192      0.233   -2329.991     568.917\n",
       "X_train[1]  -698.1691    737.843     -0.946      0.344   -2146.523     750.185\n",
       "X_train[2]     0.5258      0.034     15.523      0.000       0.459       0.592\n",
       "X_train[3]     0.8444      0.031     26.983      0.000       0.783       0.906\n",
       "X_train[4]     0.1076      0.016      6.548      0.000       0.075       0.140\n",
       "==============================================================================\n",
       "Omnibus:                     1508.437   Durbin-Watson:                   2.004\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):          2536304.512\n",
       "Skew:                          12.949   Prob(JB):                         0.00\n",
       "Kurtosis:                     277.624   Cond. No.                     3.76e+06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.76e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "df = pd.DataFrame(X_train, y_train) # Statsmodels takes only dataFrame \n",
    "reg = smf.ols(formula='y_train~X_train', data=df).fit()\n",
    "# ols -->ordinary least squares or linear regression \n",
    "reg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
